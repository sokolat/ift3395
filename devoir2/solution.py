#!/usr/bin/env python
# coding: utf-8


# Import the libraries

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


class PracticalHomework2:
    def __init__(self):
        """
        No input is required.
        """
        pass

    def generate_data(self, seed):
        """
        [DO NOT CHANGE THIS METHOD]
        Generates synthetic dataset with Gaussian mixtures and saves it as CSV files.

        Input:
        - No input required for this method. It generates synthetic data based on pre-defined parameters.

        Process:
        - Creates a dataset with 3 classes where each class is represented by samples drawn from a multivariate normal distribution.
        - The data is shuffled and split into training (60%), validation (20%), and test (20%) sets.
        - The data and labels for each split are saved as CSV files:
          - 'train_features.csv', 'train_labels.csv'
          - 'val_features.csv', 'val_labels.csv'
          - 'test_features.csv', 'test_labels.csv'

        Output:
        - No direct output is returned by this method. The generated data is saved as CSV files.
        """
        np.random.seed(seed)

        # Parameters for Gaussian mixtures
        means = [np.random.rand(10) * 2 - 1 for _ in range(3)]
        covs = [np.eye(10) * 0.5 for _ in range(3)]

        # Generate data for each class
        X_class0 = np.random.multivariate_normal(means[0], covs[0], 500)
        X_class1 = np.random.multivariate_normal(means[1], covs[1], 500)
        X_class2 = np.random.multivariate_normal(means[2], covs[2], 500)

        # Create labels
        y_class0 = np.zeros(500)
        y_class1 = np.ones(500)
        y_class2 = np.ones(500) * 2

        # Concatenate the data
        X = np.vstack([X_class0, X_class1, X_class2])
        y = np.hstack([y_class0, y_class1, y_class2])

        # Shuffle the data
        indices = np.arange(X.shape[0])
        np.random.shuffle(indices)
        X = X[indices]
        y = y[indices]

        # Split the data into training, validation, and test sets
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.4, random_state=42
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42
        )

        # Save to CSV
        pd.DataFrame(X_train).to_csv("train_features.csv", index=False)
        pd.DataFrame(y_train).to_csv("train_labels.csv", index=False)
        pd.DataFrame(X_val).to_csv("val_features.csv", index=False)
        pd.DataFrame(y_val).to_csv("val_labels.csv", index=False)
        pd.DataFrame(X_test).to_csv("test_features.csv", index=False)
        pd.DataFrame(y_test).to_csv("test_labels.csv", index=False)

    def load_and_preprocess_data(self):
        """
        [DO NOT CHANGE THIS METHOD]
        Loads data from CSV files and standardizes the features using StandardScaler.

        Input:
        - The method expects CSV files generated by the 'generate_data' method:
          - 'train_features.csv', 'train_labels.csv'
          - 'val_features.csv', 'val_labels.csv'
          - 'test_features.csv', 'test_labels.csv'

        Process:
        - Reads the CSV files containing the training, validation, and test features and labels.
        - Standardizes the features (i.e., scales them to have zero mean and unit variance) using `StandardScaler`.
        - Applies the scaling transformation to the training, validation, and test sets.

        Output:
        - Returns six NumPy arrays:
          1. `X_train`: The standardized training feature set.
          2. `y_train`: The labels for the training set.
          3. `X_val`: The standardized validation feature set.
          4. `y_val`: The labels for the validation set.
          5. `X_test`: The standardized test feature set.
          6. `y_test`: The labels for the test set.
        """
        # Load the data
        X_train = pd.read_csv("train_features.csv").values
        y_train = pd.read_csv("train_labels.csv").values.flatten()
        X_val = pd.read_csv("val_features.csv").values
        y_val = pd.read_csv("val_labels.csv").values.flatten()
        X_test = pd.read_csv("test_features.csv").values
        y_test = pd.read_csv("test_labels.csv").values.flatten()

        # Standardize the features
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_val = scaler.transform(X_val)
        X_test = scaler.transform(X_test)

        return X_train, y_train, X_val, y_val, X_test, y_test

    def make_one_versus_all_labels(self, y, num_classes):
        """
        Converts the given labels into a one-vs-all format for multi-class classification.

        Input:
        - y: Array of shape (n_samples,) containing the original class labels.
        - num_classes: Integer representing the total number of classes.

        Process:
        - Creates a binary label array where each row corresponds to one sample, and the columns
          represent one-vs-all labels for each class.

        Output:
        - Returns a label array of shape (n_samples, num_classes) with -1 for non-class columns
          and 1 for the true class column.
        """
        one_vs_all_labels = -1 * np.ones((len(y), num_classes), dtype=np.int8)
        for i, label in enumerate(np.array(y, dtype=np.int8)):
            one_vs_all_labels[i][label] += 2
        return one_vs_all_labels

    def compute_loss(self, X, y, w, C):
        """
        Computes the logistic loss for multi-class classification.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
        - w: Weight matrix of shape (n_features, n_classes).
        - C: Regularization parameter (float).

        Process:
        - Computes the loss using a logistic regression formulation.
        - Adds L2 regularization term based on the weight matrix.

        Output:
        - Returns the scalar loss value (float).
        """
        reg_terms = [0.5 * C * weight.T @ weight for weight in w.T]
        loss_per_class = (X @ w) * y
        vectorized_max = np.vectorize(lambda val: max(0, 2 - val) ** 2)
        loss_per_class = vectorized_max(loss_per_class)
        loss = (1 / y.shape[0]) * np.sum(np.sum(loss_per_class + reg_terms, axis=1))
        return loss

    def compute_gradient(self, X, y, w, C):
        """
        Computes the gradient of the logistic loss for multi-class classification.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
        - w: Weight matrix of shape (n_features, n_classes).
        - C: Regularization parameter (float).

        Process:
        - Computes the gradient of the logistic loss with respect to the weights.
        - Adds the gradient of the L2 regularization term.

        Output:
        - Returns the gradient matrix of shape (n_features, n_classes).
        """
        reg_term_grad = C * w
        sqrt_loss = (X @ w) * y
        vectorized_max = np.vectorize(lambda val: max(0, 2 - val))
        sqrt_loss = vectorized_max(sqrt_loss)
        grad_mat = (-2 / y.shape[0]) * X.T @ (sqrt_loss * y) + reg_term_grad
        return grad_mat

    def infer(self, X, w):
        """
        Predicts the class labels for a given feature matrix.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - w: Weight matrix of shape (n_features, n_classes).

        Process:
        - Computes the predicted class probabilities for each sample.
        - Assigns the class with the highest probability as the predicted class label.

        Output:
        - Returns an array of predicted class labels of shape (n_samples,).
        """
        raise NotImplementedError

    def fit(
        self,
        X_train,
        y_train,
        X_val,
        y_val,
        num_classes,
        epochs,
        learning_rate,
        C,
        batch_size,
    ):
        """
        [DO NOT CHANGE THIS METHOD]
        Fits a multi-class logistic regression model using mini-batch gradient descent.

        Input:
        - X_train: Feature matrix of shape (n_train, n_features) for training.
        - y_train: Label matrix of shape (n_train, n_classes) for training.
        - X_val: Feature matrix of shape (n_val, n_features) for validation.
        - y_val: Label matrix of shape (n_val, n_classes) for validation.
        - num_classes: Integer representing the total number of classes.
        - epochs: Integer specifying the number of training epochs.
        - learning_rate: Float value specifying the learning rate for optimization.
        - C: Regularization parameter (float).
        - batch_size: Integer specifying the mini-batch size for training.

        Process:
        - Initializes the weight matrix with zeros.
        - Iterates over the training data for the specified number of epochs.
        - Splits the training data into mini-batches and performs gradient descent.
        - Computes the training and validation losses at the end of each epoch.

        Output:
        - Returns the weight matrix after training.
        """
        num_features = X_train.shape[1]
        w = np.zeros((num_features, num_classes))

        # Dictionaries to store the metrics
        history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

        for epoch in range(epochs):
            indices = np.arange(X_train.shape[0])
            np.random.shuffle(indices)

            # Mini-batch training
            for i in range(0, X_train.shape[0], batch_size):
                batch_indices = indices[i : i + batch_size]
                X_batch = X_train[batch_indices]
                y_batch = self.make_one_versus_all_labels(
                    y_train[batch_indices], num_classes
                )

                # Compute gradients and update weights
                grad = self.compute_gradient(X_batch, y_batch, w, C)
                w -= learning_rate * grad

            # Compute training and validation loss and accuracy
            train_loss = self.compute_loss(
                X_train, self.make_one_versus_all_labels(y_train, num_classes), w, C
            )
            val_loss = self.compute_loss(
                X_val, self.make_one_versus_all_labels(y_val, num_classes), w, C
            )
            train_acc = self.compute_accuracy(X_train, y_train, w)
            val_acc = self.compute_accuracy(X_val, y_val, w)

            # Store the metrics
            history["train_loss"].append(train_loss)
            history["val_loss"].append(val_loss)
            history["train_acc"].append(train_acc)
            history["val_acc"].append(val_acc)

            print(
                f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}"
            )

    def compute_accuracy(self, X, y, w):
        """
        Computes the accuracy of predictions using the given weight matrix.

        Input:
        - X: Feature matrix of shape (n_samples, n_features).
        - y: True label vector of shape (n_samples,).
        - w: Weight matrix of shape (n_features, n_classes).

        Process:
        - Predicts the labels for the given feature matrix using the weights.
        - Compares predicted labels with true labels to compute accuracy.

        Output:
        - Returns the accuracy score (float) as a percentage of correct predictions.
        """
        raise NotImplementedError


homework = PracticalHomework2()
homework.generate_data(42)
X_train, y_train, X_val, y_val, X_test, y_test = homework.load_and_preprocess_data()

num_classes = len(np.unique(np.concatenate((y_train, y_val, y_test))))
homework.compute_gradient(
    X_train,
    homework.make_one_versus_all_labels(y_train, num_classes),
    np.ones((len(X_train[0]), 3)),
    2,
)
